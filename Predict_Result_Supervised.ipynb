{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"matches.csv\", header=0, encoding=\"latin-1\")\n",
    "\n",
    "prepared_data = pd.DataFrame()\n",
    "\n",
    "homeVenue = list()\n",
    "for i, d in data[\"team1Text\"].iteritems():\n",
    "    v = data[\"venue\"].get(i)\n",
    "    h = False\n",
    "    if isinstance(v, str) and d in v:\n",
    "        h = True\n",
    "    homeVenue.append(h)\n",
    "    \n",
    "prepared_data[\"date\"] = data.date\n",
    "prepared_data[\"name\"] = preprocessing.LabelEncoder().fit_transform(data[\"team1\"])\n",
    "prepared_data[\"opponentName\"] = preprocessing.LabelEncoder().fit_transform(data[\"team2\"])\n",
    "prepared_data[\"homeVenue\"] = homeVenue\n",
    "prepared_data[\"neutralVenue\"] = [not v for v in homeVenue]\n",
    "prepared_data[\"homeScore\"] = data.team1Score\n",
    "prepared_data[\"opponentScore\"] = data.team2Score\n",
    "prepared_data[\"differenceScore\"] = data.team1Score - data.team2Score\n",
    "prepared_data[\"homeWin\"] = data.team1Score > data.team2Score\n",
    "prepared_data[\"opponentWin\"] = data.team1Score < data.team2Score\n",
    "prepared_data[\"draw\"] = data.team1Score == data.team2Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = prepared_data.columns\n",
    "x_columns = cols.drop([\"homeScore\", \"opponentScore\", \"differenceScore\"])\n",
    "\n",
    "x = prepared_data[x_columns]\n",
    "y = prepared_data[[\"homeScore\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.416394208313872\n",
      "           date  name  opponentName  homeVenue  neutralVenue  homeWin  \\\n",
      "17099  20141118   193           143       True         False     True   \n",
      "\n",
      "       opponentWin   draw  \n",
      "17099        False  False  \n",
      "[2 2 2 2 2 2 2 0 1 1 2 2 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "clf_lr = LogisticRegression(penalty='l1', tol=0.01, C=0.1)\n",
    "print(clf_lr.fit(X_train, y_train.values.ravel()).score(X_test, y_test))\n",
    "\n",
    "X_new = X_test[0:15]\n",
    "print(X_new[0:1])\n",
    "pred = clf_lr.predict(X_new)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
